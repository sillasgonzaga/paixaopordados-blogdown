---
title: 'Anunciando um "novo" dataset no Kaggle: Pesquisa Origem Destino do Metrô SP'
author: ''
date: '2019-07-04'
slug: anunciando-um-novo-dataset-no-kaggle-pesquisa-origem-destino-do-metrô-sp
categories:
  - R
tags: []
description: ''
topics: []
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```


## Motivação

[Esta matéria da Folha de São Paulo](https://www1.folha.uol.com.br/cotidiano/2019/07/sao-paulo-ganha-nova-hora-do-rush-com-aumento-de-viagens-ao-meio-dia.shtml) me motivou a fazer algo que sempre tive muito interesse e que foi responsável por ajudar a desenvolver meu raciocínio analítico: baixar um conjunto de dados público, fazer minhas próprias análises e tirar conclusões que considero interessantes.

Ao baixar o dataset [neste link](http://www.metro.sp.gov.br/pesquisa-od/), contudo, meu primeiro obstáculo foi o formato em que os dados foram disponbilizados. Problemas como células mescladas, problemas de encoding, formato matricial (e não tidy) de tabelas, etc. Além disso, o próprio nome dos arquivos não esclarece seu conteúdo e não facilita o trabalho de quem deseja juntar as diferentes tabelas para tirar conclusões criativas. 

Por exemplo, existe relação entre renda familiar e a "popularidade" de uma zona de destino de viagens? Bairros mais ricos recebem mais trabalhadores da indústria, de serviços ou de comércio? Quais distritos de São Paulo mais "exportam" pessoas que pegam metrô para buscar emprego? As possibilidades de análises e modelagens são muitas.

Com esta motivação, ao perceber que o trabalho de limpeza de dados, realizado [neste código](https://gist.github.com/sillasgonzaga/5e3282d160ea87a97c0be0b46d00df8a), poderia ser trabalhoso para mais gente além de mim, decidi exportar os arquivos [no Kaggle](https://www.kaggle.com/sillas/pesquisa-origem-destino-2017-metr-so-paulo) em um arquivo SQLite que contem as tabelas, além de salvar as tabelas individualmente em arquivos csv após passar pelo processo de limpeza e padronização de dados.


## Como usar os dados

O `dplyr` tem um recurso muito legal: pode interagir diretamente com arquivos SQL, sejam eles como arquivo local ou como um sistema de banco de dados hospedado em algum servidor. O [Rstudio](https://db.rstudio.com/) possui um site com vários tutoriais sobre trabalhar com bancos de dados SQL.

```{r, message = FALSE, warning = FALSE}
library(RSQLite)
library(tidyverse)
# apos baixar o arquivo SQLITE chamado DB_ORIGEM_DESTINO_SP do Kaggle,
# indique abaixo o caminho para o arquivo
arquivo_db <- "/home/sillas/R/Projetos/pesquisa_origem_destino/data/DB_ORIGEM_DESTINO_SP"
# criar conexao com o banco
con <- dbConnect(RSQLite::SQLite(), dbname = arquivo_db)
```

A função `dbListTables` exibe quais tabelas estão disponíveis:

```{r}
dbListTables(con)
```

Para iniciantes nesse dataset, as tabelas mais importantes são estas:

```{r}
dbListTables(con)[c(1, 4, 5)]
```

Para interagir com uma tabela desse banco de dados, usa-se a função `tbl()`:

```{r}
tbl(con, # arquivo de conexao com o banco
    "dicionario_das_variaveis_da_tabela_dados_gerais") # nome da tabela)
```

O `dplyr` faz, então, algo que se chama de consulta lazy: o output acima não é um dataframe ou tibble, mas sim uma breve exibição das primeiras linhas do resultado da consulta à tabela. O benefício disso é que o resultado da consulta não foi carregado para a memória RAM, o que é muito útil ao lidar com bancos de dados muito grandes e queries complexas. 

No código abaixo, por exemplo, eu junto duas tabelas diferentes e filtro as zonas da cidade de São Paulo para só então "baixar" ou trazer os dados para a memória RAM com a função `collect()`:

```{r}
df_zonas_sp <- tbl(con, "dados_gerais") %>% 
  left_join(tbl(con, "dados_geograficos_das_zonas"),
            by = c("ZONA" = c("COD_ZONA"))) %>% 
  # filtrar zonas de SP
  filter(NOME_MUNICIPIO == "São Paulo") %>% 
  collect()

```

A partir dos dados obtidos, podemos fazer qualquer tipo de análise desejada. Por exemplo, um gráfico contendo a renda familiar mediana por distrito na cidade de São Paulo:

```{r, fig.width=9, fig.height=9}
# distritos por renda familiar
renda_por_distrito <- df_zonas_sp %>% 
  filter(!is.na(RENDA_FA)) %>% 
  group_by(NOME_AREA_SP_CAPITAL, NOME_DISTRITO) %>% 
  summarise(RENDA_MEDIANA = median(RENDA_FA))

quartis <- quantile(renda_por_distrito$RENDA_MEDIANA,
                    prob = c(.25, .50, .75))

renda_por_distrito %>% 
  ggplot(aes(x = fct_reorder(NOME_DISTRITO, RENDA_MEDIANA),
             y = RENDA_MEDIANA)) +
  geom_col(fill = "#A82B2EFE") +
  geom_hline(yintercept = quartis, linetype = "dashed") +
  facet_wrap(vars(NOME_AREA_SP_CAPITAL), scales = "free_y") +
  labs(x = NULL, y = NULL,
       title = "Mediana da renda familiar por zona na cidade de SP") +
  scale_y_continuous(breaks = seq(0, 10000, 2000),
                     minor_breaks = NULL) +
  coord_flip()

```

Outro tipo de análise possível pelo cruzamento de tabelas: Para onde vão as pessoas que partem da Luz?

```{r}
# para onde as pessoas que moram na luz vão?
tb_zona_de_para <- tbl(con, "dados_geograficos_das_zonas") %>% 
  select(COD_ZONA, NOME_ZONA)


tbl(con, "viagens_diarias_totais_por_zonas_de_origem_e_destino_2017") %>% 
  left_join(tb_zona_de_para, by = c("COD_ZONA_ORIGEM" = "COD_ZONA")) %>% 
  rename(NOME_ZONA_ORIGEM = NOME_ZONA) %>% 
  left_join(tb_zona_de_para, by = c("COD_ZONA_DESTINO" = "COD_ZONA")) %>% 
  rename(NOME_ZONA_DESTINO = NOME_ZONA) %>% 
  filter(NOME_ZONA_ORIGEM == "Luz") %>% 
  select(NOME_ZONA_DESTINO, QTD) %>% 
  arrange(desc(QTD))
```

## E Python?

Também é bem fácil importar um arquivo SQLite no Python combinando os módulos `sqlite3` e `pandas`.

```{python, eval = FALSE}
import sqlite3
import pandas as pd


x = '/home/sillas/R/Projetos/pesquisa_origem_destino/data/DB_ORIGEM_DESTINO_SP'
con = sqlite3.connect(x)
# importar tabela como um dataframe no pandas
tb = pd.read_sql_query('SELECT * FROM dicionario_das_variaveis_da_tabela_dados_gerais', con)


```





