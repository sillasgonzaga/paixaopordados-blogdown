---
title: 'Topic Modeling: Um algoritmo consegue entender sobre o que fala a youtuber Nathalia Arcuri?'
author: Sillas Gonzaga
date: '2018-04-14'
slug: topic-modeling-nathalia-arcuri
categories:
  - R
tags:
  - text mining
  - lexiconPT
---



<p>No meu <a href="http://www.sillasgonzaga.com/post/literaturaBR-01/">último post</a> sobre Mineração de Texto, usei algumas ferramentas do R para analisar textos clássicos da literatura brasileira. Desta vez, o foco da análise será algo mais contemporâneo: uma youtuber. Mais precisamente, a <a href="http://mepoupenaweb.uol.com.br/sobre-a-nath/">Nathalia Arcuri</a>, responsável por um dos principais canais de educação financeira, o <a href="https://www.youtube.com/channel/UC8mDF5mWNGE-Kpfcvnn0bUg/">Me Poupe</a>.</p>
<p>Além do objeto da análise, a abordagem aqui também é diferente: vou mostrar como Topic Modeling pode ser usado para descobrir temas gerais em um conjunto de dados textuais.</p>
<p>Assim, este post se dedica ao seguinte problema de pesquisa: é possível identificar, por meio de um algoritmo de inteligência artificial, temas gerais que uma youtuber com mais de 300 vídeos publicados fala sobre?</p>
<pre class="r"><code>library(reticulate)
reticulate::use_python(&quot;/home/sillas/anaconda3/bin/python&quot;, required = TRUE)
library(lexiconPT)
library(tidytext)
library(tidyverse)
library(magrittr)
library(stm)
library(tm)
library(ggridges)
library(formattable)
options(scipen = 999)</code></pre>
<div id="coleta-dos-dados" class="section level2">
<h2>Coleta dos dados</h2>
<p>Para analisar o conteúdo de vídeos de youtube, precisamos das legendas dos vídeos. Alguns (bem poucos) canais produzem suas próprias legendas manualmente, mas a grande maioria, como o Me Poupe, o canal da Nathalia Arcuri, não o faz. Sendo assim, precisamos nós mesmo produzir essas legendas.</p>
<p>Isso seria uma tarefa muito árdua, mas felizmente o próprio Youtube tem seu próprio serviço de inteligência artifical de reconhecimento de fala, que cria automaticamente legendas para um vídeo. Apesar de algumas vezes as legendas produzidas pelo algoritmo do Youtube não serem muito fiéis, elas serão usadas como dados brutos para a modelagem por tópicos. Os resultados apresentados no post mostram que essas legendas automáticas podem sim serem usadas para fins de estudo.</p>
<div class="figure">
<img src="https://i.imgur.com/dQFxp9g.png?1" alt="Nathalia Arcuri" style="width:100.0%" />
<p class="caption">Nathalia Arcuri</p>
</div>
<p>Para coletar as legendas dos vídeo, eu uso um utilitário de linha de comando chamado <a href="https://rg3.github.io/youtube-dl/"><code>youtube-dl</code></a>, que é bem simples de usar. No código abaixo, que mistura R com shell script, eu mostro como montar uma query para baixar as legendas do vídeo em arquivos de texto cujos nomes contem alguns metadados do vídeo, descritos na variável <code>fields_raw</code>. Caso você deseje realizar este mesmo estudo com outro youtuber, basta atribuir a url do canal na variável <code>channel_url</code>.</p>
<pre class="r"><code>fields_raw &lt;- c(&quot;id&quot;, &quot;title&quot;, &quot;alt_title&quot;, &quot;creator&quot;, &quot;release_date&quot;,
                &quot;timestamp&quot;, &quot;upload_date&quot;, &quot;duration&quot;, &quot;view_count&quot;,
                &quot;like_count&quot;, &quot;dislike_count&quot;, &quot;comment_count&quot;)

fields &lt;- fields_raw %&gt;% 
  map_chr(~paste0(&quot;%(&quot;, ., &quot;)s&quot;)) %&gt;% 
  # usar &amp;&amp;&amp; como separador de fields
  paste0(collapse = &quot;&amp;&amp;&amp;&quot;) %&gt;% 
  # acrescentar aspas no inicio e no final do string
  paste0(&#39;&quot;&#39;, ., &#39;&quot;&#39;)

# canal do me poupe
channel_url &lt;- &quot;https://www.youtube.com/channel/UC8mDF5mWNGE-Kpfcvnn0bUg&quot;

# montar query (comando) do youtube-dl
cmd_ytdl &lt;- str_glue(&quot;youtube-dl -o {fields} -i -v -w --skip-download --write-auto-sub --sub-lang pt {channel_url}&quot;)

# acrescentar diretorio
pasta_captions &lt;- &quot;/home/sillas/R/Projetos/paixaopordados-blogdown/data/mepoupe&quot;
fs::dir_create(pasta_captions)
cmd &lt;- str_glue(&quot;cd {pasta_captions} &amp;&amp; {cmd_ytdl}&quot;)

# imprimir comando para ver como ficou
cat(cmd)</code></pre>
<pre><code>## cd /home/sillas/R/Projetos/paixaopordados-blogdown/data/mepoupe &amp;&amp; youtube-dl -o &quot;%(id)s&amp;&amp;&amp;%(title)s&amp;&amp;&amp;%(alt_title)s&amp;&amp;&amp;%(creator)s&amp;&amp;&amp;%(release_date)s&amp;&amp;&amp;%(timestamp)s&amp;&amp;&amp;%(upload_date)s&amp;&amp;&amp;%(duration)s&amp;&amp;&amp;%(view_count)s&amp;&amp;&amp;%(like_count)s&amp;&amp;&amp;%(dislike_count)s&amp;&amp;&amp;%(comment_count)s&quot; -i -v -w --skip-download --write-auto-sub --sub-lang pt https://www.youtube.com/channel/UC8mDF5mWNGE-Kpfcvnn0bUg</code></pre>
<p>Portanto, temos o comando para baixar as legendas dos vídeos na pasta indicada. Para rodar o comando, basta a variável <code>cmd</code> como argumento da função <code>system()</code> ou o copiar e colar no terminal.</p>
<p>Uma amostra dos arquivos baixados:</p>
<pre class="r"><code>dir(pasta_captions)[1:3]</code></pre>
<pre><code>## [1] &quot;039orzgCUt0&amp;&amp;&amp;10 DILEMAS MAIS FREQUENTES SOBRE TESOURO DIRETO! Tire as dúvidas e invista!&amp;&amp;&amp;NA&amp;&amp;&amp;NA&amp;&amp;&amp;NA&amp;&amp;&amp;NA&amp;&amp;&amp;20170424&amp;&amp;&amp;642&amp;&amp;&amp;272848&amp;&amp;&amp;33970&amp;&amp;&amp;137&amp;&amp;&amp;NA.pt.vtt&quot;                 
## [2] &quot;0bqZrSaitDo&amp;&amp;&amp;PLANILHA DE INDEPENDÊNCIA FINANCEIRA! Aprenda a usar! Série especial_ Office 365 #AjudaaNath&amp;&amp;&amp;NA&amp;&amp;&amp;NA&amp;&amp;&amp;NA&amp;&amp;&amp;NA&amp;&amp;&amp;20170328&amp;&amp;&amp;988&amp;&amp;&amp;260474&amp;&amp;&amp;18439&amp;&amp;&amp;206&amp;&amp;&amp;NA.pt.vtt&quot;
## [3] &quot;0eSSqsHSr2A&amp;&amp;&amp;Série empreendedorismo na veia Ep 3_ EXPERIENTES TAMBÉM FALHAM!&amp;&amp;&amp;NA&amp;&amp;&amp;NA&amp;&amp;&amp;NA&amp;&amp;&amp;NA&amp;&amp;&amp;20161225&amp;&amp;&amp;282&amp;&amp;&amp;32361&amp;&amp;&amp;2307&amp;&amp;&amp;21&amp;&amp;&amp;NA.pt.vtt&quot;</code></pre>
</div>
<div id="limpeza-dos-dados" class="section level2">
<h2>Limpeza dos dados</h2>
<p>O Youtube tem um formato próprio de arquivos para legendas chamado vtt. Veja como são os arquivos de texto baixados na etapa anterior:</p>
<pre class="r"><code>arquivos_captions &lt;- dir(pasta_captions, pattern = &#39;*.vtt&#39;, full.names = TRUE)
amostra &lt;- arquivos_captions[1]

read_lines(amostra)[1:30]</code></pre>
<pre><code>##  [1] &quot;WEBVTT&quot;                                                                                                                                                                                                    
##  [2] &quot;Kind: captions&quot;                                                                                                                                                                                            
##  [3] &quot;Language: pt&quot;                                                                                                                                                                                              
##  [4] &quot;Style:&quot;                                                                                                                                                                                                    
##  [5] &quot;::cue(c.colorCCCCCC) { color: rgb(204,204,204);&quot;                                                                                                                                                           
##  [6] &quot; }&quot;                                                                                                                                                                                                        
##  [7] &quot;::cue(c.colorE5E5E5) { color: rgb(229,229,229);&quot;                                                                                                                                                           
##  [8] &quot; }&quot;                                                                                                                                                                                                        
##  [9] &quot;##&quot;                                                                                                                                                                                                        
## [10] &quot;&quot;                                                                                                                                                                                                          
## [11] &quot;00:00:00.000 --&gt; 00:00:02.419 align:start position:0%&quot;                                                                                                                                                     
## [12] &quot; &quot;                                                                                                                                                                                                         
## [13] &quot;olha&lt;00:00:00.329&gt;&lt;c&gt; o&lt;/c&gt;&lt;00:00:00.420&gt;&lt;c&gt; data&lt;/c&gt;&lt;c.colorE5E5E5&gt;&lt;00:00:00.840&gt;&lt;c&gt; mtow&lt;/c&gt;&lt;00:00:01.260&gt;&lt;c&gt; que&lt;/c&gt;&lt;00:00:01.350&gt;&lt;c&gt; apurou&lt;/c&gt;&lt;00:00:01.709&gt;&lt;c&gt; que&lt;/c&gt;&lt;00:00:01.979&gt;&lt;c&gt; esta&lt;/c&gt;&lt;/c&gt;&quot;
## [14] &quot;&quot;                                                                                                                                                                                                          
## [15] &quot;00:00:02.419 --&gt; 00:00:02.429 align:start position:0%&quot;                                                                                                                                                     
## [16] &quot;olha o data&lt;c.colorE5E5E5&gt; mtow que apurou que esta&quot;                                                                                                                                                       
## [17] &quot; &lt;/c&gt;&quot;                                                                                                                                                                                                     
## [18] &quot;&quot;                                                                                                                                                                                                          
## [19] &quot;00:00:02.429 --&gt; 00:00:04.640 align:start position:0%&quot;                                                                                                                                                     
## [20] &quot;olha o data&lt;c.colorE5E5E5&gt; mtow que apurou que esta&lt;/c&gt;&quot;                                                                                                                                                   
## [21] &quot;&lt;c.colorCCCCCC&gt;pergunta&lt;/c&gt;&lt;c.colorE5E5E5&gt;&lt;00:00:03.210&gt;&lt;c&gt; este&lt;/c&gt;&lt;00:00:03.389&gt;&lt;c&gt; dilema&lt;/c&gt;&lt;00:00:03.780&gt;&lt;c&gt; é&lt;/c&gt;&lt;00:00:04.170&gt;&lt;c&gt; o&lt;/c&gt;&lt;00:00:04.259&gt;&lt;c&gt; mais&lt;/c&gt;&lt;/c&gt;&quot;                              
## [22] &quot;&quot;                                                                                                                                                                                                          
## [23] &quot;00:00:04.640 --&gt; 00:00:04.650 align:start position:0%&quot;                                                                                                                                                     
## [24] &quot;&lt;c.colorCCCCCC&gt;pergunta&lt;/c&gt;&lt;c.colorE5E5E5&gt; este dilema é o mais&quot;                                                                                                                                           
## [25] &quot; &lt;/c&gt;&quot;                                                                                                                                                                                                     
## [26] &quot;&quot;                                                                                                                                                                                                          
## [27] &quot;00:00:04.650 --&gt; 00:00:06.650 align:start position:0%&quot;                                                                                                                                                     
## [28] &quot;&lt;c.colorCCCCCC&gt;pergunta&lt;/c&gt;&lt;c.colorE5E5E5&gt; este dilema é o mais&lt;/c&gt;&quot;                                                                                                                                       
## [29] &quot;&lt;c.colorE5E5E5&gt;freqüente&lt;00:00:05.190&gt;&lt;c&gt; de&lt;/c&gt;&lt;00:00:05.310&gt;&lt;c&gt; tudo&lt;/c&gt;&lt;00:00:05.700&gt;&lt;c&gt; o&lt;/c&gt;&lt;00:00:05.790&gt;&lt;c&gt; que&lt;/c&gt;&lt;/c&gt;&lt;c.colorCCCCCC&gt;&lt;00:00:05.970&gt;&lt;c&gt; investiu&lt;/c&gt;&lt;00:00:06.509&gt;&lt;c&gt; uma&lt;/c&gt;&lt;/c&gt;&quot;  
## [30] &quot;&quot;</code></pre>
<p>Temos vários problemas de dados não estruturados aí, como timestamps, formatação de estilo como cor e alinhamento e repetição de frases em diferentes linhas (note como as frases nas linhas 24 e 28 são as mesmas). Entretanto, limpar esses dados é mais simples que se imagina. Segue o passo-a-passo:</p>
<p>Começando pelo mais crítico, vamos remover toda a formatação de legendas do texto, deixando apenas as frases. Para isso, uso um módulo Python (sim, eu me rendi, esta é a primeira vez que uso Python no blog) chamado <code>webvtt</code>:</p>
<pre class="python"><code>from webvtt import WebVTT
def caption_to_vector(file):
  x = WebVTT().read(file)
  txt = [caption.text for caption in x]
  return(txt)</code></pre>
<p>Vejam com essa função faz todo o trabalho bruto de limpar os metadados da legenda:</p>
<pre class="r"><code>x &lt;- amostra %&gt;% caption_to_vector()
x[1:20]</code></pre>
<pre><code>##  [1] &quot; \nolha o data mtow que apurou que esta&quot;                                      
##  [2] &quot;olha o data mtow que apurou que esta\n &quot;                                      
##  [3] &quot;olha o data mtow que apurou que esta\npergunta este dilema é o mais&quot;          
##  [4] &quot;pergunta este dilema é o mais\n &quot;                                             
##  [5] &quot;pergunta este dilema é o mais\nfreqüente de tudo o que investiu uma&quot;          
##  [6] &quot;freqüente de tudo o que investiu uma\n &quot;                                      
##  [7] &quot;freqüente de tudo o que investiu uma\nvez só ou possa investir todo mês&quot;      
##  [8] &quot;vez só ou possa investir todo mês\n &quot;                                         
##  [9] &quot;vez só ou possa investir todo mês\nfilho olha esse aqui se não ficar nos&quot;     
## [10] &quot;filho olha esse aqui se não ficar nos\n &quot;                                     
## [11] &quot;filho olha esse aqui se não ficar nos\nvídeos em alta do yuan subir é&quot;        
## [12] &quot;vídeos em alta do yuan subir é\n &quot;                                            
## [13] &quot;vídeos em alta do yuan subir é\nmarmelada hoje vou falar para você&quot;           
## [14] &quot;marmelada hoje vou falar para você\n &quot;                                        
## [15] &quot;marmelada hoje vou falar para você\nquais são os dez maiores dilemas de&quot;      
## [16] &quot;quais são os dez maiores dilemas de\n &quot;                                       
## [17] &quot;quais são os dez maiores dilemas de\nquem quer investir no tesouro direto mas&quot;
## [18] &quot;quem quer investir no tesouro direto mas\n &quot;                                  
## [19] &quot;quem quer investir no tesouro direto mas\nainda não sabe muito bem como fazer&quot;
## [20] &quot;ainda não sabe muito bem como fazer\n &quot;</code></pre>
<p>Ainda temos alguns problemas, como as linhas repetidas, mas isso é o de menos. Resolvemos todos os problemas de limpeza de dados com a função abaixo:</p>
<pre class="r"><code>limpar_caption &lt;- function(arquivo){
  caption_raw &lt;- caption_to_vector(arquivo)
  n &lt;- length(caption_raw)
  # remover \n das linhas com exceção da ultima
  caption &lt;- c(stringr::str_remove_all(caption_raw[-n], &quot;[\n].*&quot;),
               caption_raw[n])
  # remover duplicatas
  caption &lt;- unique(caption)
  # remover acentos
  caption &lt;- iconv(caption, from = &quot;UTF-8&quot;, to = &quot;ASCII//TRANSLIT&quot;)
  # juntar todo o vector em um so
  caption &lt;- paste0(caption, collapse = &quot;\n&quot;)
  caption
}

# exemplo
# amostra %&gt;% limpar_caption()</code></pre>
<p>Além disso, precisamos também extrair os metadados dos vídeos salvos nos nomes dos arquivos:</p>
<pre class="r"><code># funcao para extrair metadados do video baseado no titulo
extrair_metadados &lt;- function(arquivo, pasta = pasta_captions,
                              fields = fields_raw){
  mat &lt;- str_split(arquivo, &quot;&amp;{3}&quot;, simplify = TRUE)
  # substituir elemento da primeira coluna por id (remover pasta do nome)
  mat[1,1] &lt;- mat[1,1] %&gt;% str_remove_all(pasta) %&gt;% str_remove_all(&quot;/&quot;)
  
  # renomear colunas
  cols &lt;- fields[1:ncol(mat)]
  colnames(mat) &lt;- cols
  as.tibble(mat)
}</code></pre>
<p>Finalmente, a função abaixo cria um dataframe com as colunas de metadados e de legenda, que chamo de caption:</p>
<pre class="r"><code># funcao para juntar tudo num dataframe so
caption_to_df &lt;- function(arquivo, ...){
  
  caption &lt;- limpar_caption(arquivo)
  meta &lt;- extrair_metadados(arquivo, ...)
  meta &lt;- meta %&gt;% mutate(caption = caption)
  
  meta
}

### gerar dataframe para todos os videos
df &lt;- arquivos_captions %&gt;% 
  map_df(caption_to_df) %&gt;% 
  # remover coluna que nao uso
  select(-comment_count) %&gt;% 
  # converter a classe de algumas colunas
  mutate(upload_date = lubridate::ymd(upload_date)) %&gt;% 
  mutate_at(vars(duration:dislike_count), as.numeric)

# vendo como ficou
glimpse(df)</code></pre>
<pre><code>## Observations: 319
## Variables: 12
## $ id            &lt;chr&gt; &quot;039orzgCUt0&quot;, &quot;0bqZrSaitDo&quot;, &quot;0eSSqsHSr2A&quot;, &quot;0G...
## $ title         &lt;chr&gt; &quot;10 DILEMAS MAIS FREQUENTES SOBRE TESOURO DIRETO...
## $ alt_title     &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, ...
## $ creator       &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, ...
## $ release_date  &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, ...
## $ timestamp     &lt;chr&gt; &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, &quot;NA&quot;, ...
## $ upload_date   &lt;date&gt; 2017-04-24, 2017-03-28, 2016-12-25, 2016-02-15,...
## $ duration      &lt;dbl&gt; 642, 988, 282, 440, 670, 647, 574, 396, 615, 614...
## $ view_count    &lt;dbl&gt; 272848, 260474, 32361, 242366, 262462, 53146, 11...
## $ like_count    &lt;dbl&gt; 33970, 18439, 2307, 13163, 14439, 3868, 9369, 17...
## $ dislike_count &lt;dbl&gt; 137, 206, 21, 371, 453, 89, 110, 176, 87, 149, 1...
## $ caption       &lt;chr&gt; &quot; \nolha o data mtow que apurou que esta\npergun...</code></pre>
</div>
<div id="topic-modeling" class="section level2">
<h2>Topic Modeling</h2>
<p>Antes de partir para o código aplicado a Topic Modeling, uma brevíssima introdução teórica.</p>
<p>No contexto de <a href="https://en.wikipedia.org/wiki/Text_mining">Text Mining</a>, um <strong>tópico</strong> <span class="math inline">\(T\)</span> é definido como um conjunto de <strong>palavras</strong> ou tokens (<span class="math inline">\(w_1, w_2, ... w_n\)</span>) onde cada palavra possui uma probabilidade de pertencer a um tópico, e um <strong>documento</strong> é definido como um conjunto de tópicos, onde cada um possui uma proporção de presença dentro do documento. A soma da proporção de cada tópico dentro de um documento é igual a 1, assim como a soma das probabilidades de cada palavra para um dado tópico, que também é igual a 1. Nesta análise, um documento corresponde a cada vídeo lançado no canal Me Poupe.</p>
<p>Topic Modeling, portanto, é definido como uma ténica não-supervisionada de Machine Learning que busca que identica clusteres ou grupos de palavras que ocorrem juntas, descobrindo assim tópicos abstratos que ocorrem em um conjunto de documentos. O objetivo é descobrir subestruturas semânticas dentro de um conjunto de textos.</p>
<p>Existem alguns algoritmos de Topic Modeling, muitos deles presentes no pacote <code>topicmodels</code>. Apenas devido a preferência pessoal, vou usar neste post o algoritmo <strong>Structural Topic Models</strong> (STM), presente no pacote <code>stm</code>.</p>
<p>A qualidade de um tópico encontrado por um algoritmo pode ser medida por algumas métricas, como <strong>exclusividade</strong> e <strong>coerência semântica</strong>, cuja ideia é que, em um modelo de tópicos que é semanticamente coerente, as palavras que são mais prováveis de pertencer a um tópico devem ocorrer dentro de um mesmo documento. A formulação matemática dessas métricas é razoavelmente mais complicadas que essas explicações. Caso você deseja conhecer essas métricas com mais detalhes, confira as referências no final do post.</p>
<div id="pre-processamento" class="section level3">
<h3>Pré-processamento</h3>
<p>Mesmo após o processo de limpeza, é necessário realizar alguns pré-processamentos antes de aplicar o algoritmo de Topic Modeling.</p>
<p>Um dos procedimentos necessários é a remoção de stopwords, que são palavras que ocorrem tão frequentemente que não acrescentam nenhum valor semântico a um texto, como “e”, “vai”, “lá”, etc. O pacote <code>tm</code> fornece uma lista de stopwords em vários idiomas, incluindo Português.</p>
<p>O código abaixo retorna as palavras mais faladas nos vídeos do Me Poupe, após a remoção de stopwords:</p>
<pre class="r"><code># stopwords da lingua portuguesa sem acento
sw_pt_tm &lt;- tm::stopwords(&quot;pt&quot;) %&gt;% iconv(from = &quot;UTF-8&quot;, to = &quot;ASCII//TRANSLIT&quot;)

# criar dataframe com uma linha por palavra
df_palavra &lt;- df %&gt;% 
  unnest_tokens(palavra, caption) %&gt;% 
  # filtrar fora stopword
  filter(!palavra %in% sw_pt_tm)

df_palavra %&gt;% 
  count(palavra) %&gt;% 
  arrange(desc(n)) %&gt;% 
  head(20) %&gt;% 
  formattable()</code></pre>
<table class="table table-condensed">
<thead>
<tr>
<th style="text-align:right;">
palavra
</th>
<th style="text-align:right;">
n
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
vai
</td>
<td style="text-align:right;">
5467
</td>
</tr>
<tr>
<td style="text-align:right;">
gente
</td>
<td style="text-align:right;">
3297
</td>
</tr>
<tr>
<td style="text-align:right;">
porque
</td>
<td style="text-align:right;">
3077
</td>
</tr>
<tr>
<td style="text-align:right;">
aqui
</td>
<td style="text-align:right;">
3047
</td>
</tr>
<tr>
<td style="text-align:right;">
pra
</td>
<td style="text-align:right;">
3001
</td>
</tr>
<tr>
<td style="text-align:right;">
dinheiro
</td>
<td style="text-align:right;">
2731
</td>
</tr>
<tr>
<td style="text-align:right;">
fazer
</td>
<td style="text-align:right;">
2373
</td>
</tr>
<tr>
<td style="text-align:right;">
entao
</td>
<td style="text-align:right;">
2256
</td>
</tr>
<tr>
<td style="text-align:right;">
pode
</td>
<td style="text-align:right;">
2119
</td>
</tr>
<tr>
<td style="text-align:right;">
la
</td>
<td style="text-align:right;">
2097
</td>
</tr>
<tr>
<td style="text-align:right;">
ter
</td>
<td style="text-align:right;">
1645
</td>
</tr>
<tr>
<td style="text-align:right;">
ai
</td>
<td style="text-align:right;">
1614
</td>
</tr>
<tr>
<td style="text-align:right;">
agora
</td>
<td style="text-align:right;">
1520
</td>
</tr>
<tr>
<td style="text-align:right;">
ser
</td>
<td style="text-align:right;">
1516
</td>
</tr>
<tr>
<td style="text-align:right;">
pessoas
</td>
<td style="text-align:right;">
1354
</td>
</tr>
<tr>
<td style="text-align:right;">
canal
</td>
<td style="text-align:right;">
1334
</td>
</tr>
<tr>
<td style="text-align:right;">
vou
</td>
<td style="text-align:right;">
1334
</td>
</tr>
<tr>
<td style="text-align:right;">
hoje
</td>
<td style="text-align:right;">
1319
</td>
</tr>
<tr>
<td style="text-align:right;">
video
</td>
<td style="text-align:right;">
1267
</td>
</tr>
<tr>
<td style="text-align:right;">
tudo
</td>
<td style="text-align:right;">
1243
</td>
</tr>
</tbody>
</table>
<p>Como muitas dessas palavras não possuem um grande valor semântico para a separação de tópicos, podemos as acrescentar à lista de stopwords.</p>
<pre class="r"><code>minhas_sw &lt;- c(&quot;vai&quot;, &quot;porque&quot;, &quot;vou&quot;, &quot;ai&quot;, &quot;pra&quot;, &quot;entao&quot;)
sw_pt &lt;- c(minhas_sw, sw_pt_tm)</code></pre>
<p>Partimos então para o processamento de textos com as funções do pacote <code>stm</code>.</p>
<pre class="r"><code>proc &lt;- stm::textProcessor(df$caption, metadata = df, language = &quot;portuguese&quot;,
                           customstopwords = sw_pt)</code></pre>
<pre><code>## Building corpus... 
## Converting to Lower Case... 
## Removing punctuation... 
## Removing stopwords... 
## Remove Custom Stopwords...
## Removing numbers... 
## Stemming... 
## Creating Output...</code></pre>
<pre class="r"><code>out &lt;- stm::prepDocuments(proc$documents, proc$vocab, proc$meta,
                          lower.thresh = 10)</code></pre>
<pre><code>## Removing 14665 of 16857 terms (33357 of 129408 tokens) due to frequency 
## Your corpus now has 319 documents, 2192 terms and 96051 tokens.</code></pre>
</div>
</div>
<div id="criacao-do-modelo-de-topicos-e-interpretacao-dos-resultados" class="section level2">
<h2>Criação do modelo de tópicos e interpretação dos resultados</h2>
<p>A quantidade de tópicos <span class="math inline">\(K\)</span> que desejamos extrair dos textos é, na verdade, escolhida pelo ser humano. Não há uma regra precisa que defina o número ótimo de clusteres. No entanto, é possível usar a função <code>stm::searchK</code> para rodar o STM para diferentes valores do parâmetro <code>K</code> para encontrar o valor que otimiza a exclusividade e a coerência semântica do modelo.</p>
<pre class="r"><code>storage &lt;- stm::searchK(out$documents, out$vocab, K = c(3:15),
                          data = out$meta)</code></pre>
<p>Após uma inspeção manual, decidi usar <span class="math inline">\(K = 10\)</span> para encontrar 10 tópicos sobre os quais a Nathalia Arcuri mais fala.</p>
<pre class="r"><code>fit &lt;- stm(
  documents = out$documents, vocab = out$vocab, data = out$meta,  K = 10,
  max.em.its = 75, init.type = &quot;Spectral&quot;, verbose = FALSE
  )</code></pre>
<p>Após a construção do modelo, a melhor forma de visualizar os resultados é por meio de um gráfico:</p>
<pre class="r"><code>plot(fit, &quot;summary&quot;)</code></pre>
<p><img src="/post/2018-04-14-topic-modeling-nathalia-arcuri_files/figure-html/unnamed-chunk-10-1.png" width="864" /></p>
<p>Ou mesmo por texto:</p>
<pre class="r"><code>stm::labelTopics(fit)</code></pre>
<pre><code>## Topic 1 Top Words:
##       Highest Prob: gent, aqui, coisa, fazer, pode, assim, casa 
##       FREX: roupa, cabelo, peca, barato, economizar, comer, loja 
##       Lift: energia, cor, tratamento, sapato, ovo, armario, compro 
##       Score: energia, tratamento, ovo, cabelo, pared, loja, dica 
## Topic 2 Top Words:
##       Highest Prob: gent, aqui, pessoa, pergunta, agora, vamo, fazer 
##       FREX: milhao, hashtag, galera, pergunta, poup, job, inscrito 
##       Lift: bilhao, premio, jornalista, seguidor, contrata, hashtag, orgulho 
##       Score: bilhao, live, job, milhao, premio, hashtag, conteudo 
## Topic 3 Top Words:
##       Highest Prob: dinheiro, reai, ano, aqui, fazer, video, mil 
##       FREX: meta, planilha, tarefa, viver, aposentadoria, reai, poupar 
##       Lift: vitoria, offic, aposentar, caderno, planilha, metodo, produtividad 
##       Score: vitoria, caderno, meta, offic, planilha, joaquina, metodo 
## Topic 4 Top Words:
##       Highest Prob: dinheiro, coisa, mulher, vida, pessoa, pode, ser 
##       FREX: mulher, mae, present, crianca, casamento, maravilha, pai 
##       Lift: gostam, casamento, adulto, maravilha, honesto, casar, present 
##       Score: gostam, casamento, natal, present, maravilha, adulto, objetivo 
## Topic 5 Top Words:
##       Highest Prob: gent, cidad, aqui, rio, janeiro, fazer, candidato 
##       FREX: cidad, candidato, prefeitura, janeiro, rio, prefeito, municipio 
##       Lift: arrecadou, evasiva, habitacao, municipio, prefeitura, promovida, tocar 
##       Score: municipio, tocar, candidato, prefeitura, habitacao, corrupcao, prefeito 
## Topic 6 Top Words:
##       Highest Prob: tesouro, dinheiro, taxa, fundo, direto, aqui, investimento 
##       FREX: tesouro, selic, fundo, direto, taxa, titulo, corretora 
##       Lift: digit, garantidor, imobiliario, ipc, ipc-, cdi, tesouro 
##       Score: digit, tesouro, selic, imobiliario, corretora, taxa, rentabilidad 
## Topic 7 Top Words:
##       Highest Prob: empresa, pessoa, aqui, pode, fazer, dinheiro, valor 
##       FREX: aco, aplicativo, empresa, aluguel, mercado, bolsa, acao 
##       Lift: placa, aco, lucro, bolo, alugar, documento, moeda 
##       Score: placa, aco, investidor, alugar, bolo, empresa, lucro 
## Topic 8 Top Words:
##       Highest Prob: ponto, cartao, gent, credito, aqui, pode, fazer 
##       FREX: multiplus, ponto, acumular, multiplo, cartao, black, site 
##       Lift: milha, anuidad, multiplus, multiplo, passagen, acumular, acumula 
##       Score: multiplus, anuidad, multiplo, ponto, black, acumular, cartao 
## Topic 9 Top Words:
##       Highest Prob: divida, dinheiro, pagar, carro, credito, cartao, pode 
##       FREX: divida, carro, parcela, pagar, juro, credito, ajuda 
##       Lift: chequ, financiado, quitar, devendo, divida, parcelar, parcela 
##       Score: chequ, divida, credito, cartao, financiamento, parcela, consorcio 
## Topic 10 Top Words:
##       Highest Prob: pessoa, gent, fazer, ser, aqui, dinheiro, ter 
##       FREX: ingl, empreendedor, curso, faculdad, sucesso, segredo, negocio 
##       Lift: encontrei, mestr, ingl, atividad, consequencia, tecnico, milionaria 
##       Score: encontrei, live, faculdad, empreendedor, conteudo, ingl, chefe</code></pre>
<p>Na tabela acima, existe para cada tópico uma lista de palavras-chave, de acordo com uma métrica de associação. As métricas mais interessantes a serem olhadas são <strong>Highest prob</strong>, que é basicamente uma contagem de cada palavra no tópico, e <strong>FREX</strong>, que é combina frequência e exclusividade para identificar palavras que mais representam o tópico.</p>
<p>Assim, a interpretação dos tópicos pode ser feita desta maneira:</p>
<ul>
<li>Tópico 1: Dicas de economia doméstica, como que para reduzir despesas em casa;<br />
</li>
<li>Tópico 2: Genérico;<br />
</li>
<li>Tópico 3: Planejamento Financeiro, com temas como planilhas e aposentadoria;<br />
</li>
<li>Tópico 4: Vídeos focados para mulheres ou famílias;<br />
</li>
<li>Tópico 5: Política;<br />
</li>
<li>Tópico 6: Renda Fixa (este ficou bem claro);<br />
</li>
<li>Tópico 7: Dificil saber. Talvez renda variável;<br />
</li>
<li>Tópico 8: Cartão de cŕedito e temas afins, como programas de milhas;<br />
</li>
<li>Tópico 9: Dívidas e despesas;</li>
<li>Tópico 10: Empreendedorimo.</li>
</ul>
<p>É possível representar visualmente a diferença entre um par de tópicos, como o 6 e o 9:</p>
<pre class="r"><code>plot(fit, &quot;perspective&quot;, topics = c(9, 6))</code></pre>
<p><img src="/post/2018-04-14-topic-modeling-nathalia-arcuri_files/figure-html/unnamed-chunk-12-1.png" width="864" /></p>
<p>Percebe-se que as palavras “dívida” e “tesouro” separam bem esses dois tópicos.</p>
<div id="atribuicao-de-topicos-a-videos" class="section level3">
<h3>Atribuição de tópicos a vídeos</h3>
<p>Um dos possíveis produtos da análise de Topic Modeling é atribuir a cada documento (ou vídeo) um tópico, de acordo com a proporção de cada tópico nele.</p>
<p>O objeto <code>fit</code> possui internamente um elemento que é uma matriz <span class="math inline">\(N \times K\)</span>, onde N é o número de documentos. Assim, para cada vídeo, existe um porcentual de participação de cada tópico, cuja soma é igual a 1.</p>
<p>Vejamos essa matriz para os cinco primeiros vídeos:</p>
<pre class="r"><code>head(fit$theta)</code></pre>
<pre><code>##             [,1]        [,2]        [,3]         [,4]         [,5]
## [1,] 0.000273920 0.003253428 0.004040834 0.0009959664 0.0001388210
## [2,] 0.018310800 0.030741680 0.708967915 0.0118358411 0.0006963748
## [3,] 0.070843633 0.041732621 0.013845361 0.0304166112 0.0839133871
## [4,] 0.008194601 0.002381194 0.008008110 0.0033322595 0.0018488345
## [5,] 0.010782705 0.026464694 0.010263833 0.0477243086 0.1095574639
## [6,] 0.006023939 0.872040010 0.005714869 0.0059672704 0.0481083479
##             [,6]        [,7]         [,8]        [,9]        [,10]
## [1,] 0.985509536 0.002881037 0.0001246733 0.002037421 0.0007443629
## [2,] 0.183050228 0.005301315 0.0292175603 0.008811973 0.0030663133
## [3,] 0.010378165 0.176350442 0.0412429834 0.262069287 0.2692075093
## [4,] 0.001888864 0.106774265 0.8546355488 0.011153510 0.0017828124
## [5,] 0.047119628 0.504319587 0.0113556114 0.156215296 0.0761968712
## [6,] 0.019619518 0.011139023 0.0019899886 0.006333920 0.0230631152</code></pre>
<p>Observando a primeira linha, nota-se que existe um claro predomínio do Tópico 6, que representa 98% da distribuição de tópicos. Qual é esse vídeo?</p>
<pre class="r"><code>df %&gt;% 
  filter(row_number() == 1) %&gt;% 
  select(id, title)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   id          title                                                       
##   &lt;chr&gt;       &lt;chr&gt;                                                       
## 1 039orzgCUt0 10 DILEMAS MAIS FREQUENTES SOBRE TESOURO DIRETO! Tire as dú…</code></pre>
<p>Vemos que o algoritmo acertou na mosca, pois de fato o vídeo é sobre Renda Fixa.</p>
<p>A qual tópico pertence o vídeo mais assistido do Me Poupe?</p>
<pre class="r"><code># id do video Meu Primeiro Milhao
ind &lt;- which(df$id == &quot;4jaWDfTbytA&quot;)
t(fit$theta[ind,])</code></pre>
<pre><code>##            [,1]       [,2]      [,3]       [,4]         [,5]        [,6]
## [1,] 0.05483404 0.03399528 0.5101044 0.01345342 0.0007212358 0.003021568
##             [,7]        [,8]        [,9]     [,10]
## [1,] 0.003167582 0.001051847 0.003699136 0.3759515</code></pre>
<p>Os tópicos predominantes são 3 (Planejamento financeiro), com probabilidade de 51% e 10 (empreendedorismo), com 38%. Faz sentido, porque neste vídeo a Nathalia conta sua história como empreendedora para atingir seu primeiro milhão por meio de muito planejamento financeiro (não tô forçando, eu vi o vídeo).</p>
<p>Vamos então, para cada vídeo, extrair seu tópico predominante e contar a frequência de videos por tópico:</p>
<pre class="r"><code>nomes_topicos &lt;- c(&quot;economia_domestica&quot;, &quot;generico&quot;, &quot;plan_financeiro&quot;,
                   &quot;mulher_familia&quot;, &quot;politica&quot;, &quot;renda_fixa&quot;, &quot;renda_variavel&quot;,
                   &quot;cartao_de_credito&quot;, &quot;dividas&quot;, &quot;empreendedorismo&quot;)
# extrair a maior probabilidade pra cada video
maior_prob &lt;- apply(fit$theta, 1, max)
# extrair o nome do topico com a maior probabilidade
topico_video &lt;- nomes_topicos[apply(fit$theta, 1, which.max)]

# acrescentar esses dados no dataframe principal
df_topico &lt;- df %&gt;% 
  mutate(maior_prob = maior_prob,
         topico = topico_video)</code></pre>
<pre class="r"><code>roxo &lt;- &quot;mediumpurple4&quot;

# grafico da quantidade de videos por topico
df_topico %&gt;% 
  count(topico) %&gt;% 
  # classificar em ordem decrescente
  mutate(topico = forcats::fct_reorder(topico, n)) %&gt;% 
  ggplot(aes(x = topico, y = n)) + 
  geom_col(fill = roxo) +
  theme_minimal() + 
  labs(x = NULL, y = &quot;Vídeos&quot;,
       title = &quot;Quantidade de vídeos por tópico&quot;) +
  coord_flip()</code></pre>
<p><img src="/post/2018-04-14-topic-modeling-nathalia-arcuri_files/figure-html/unnamed-chunk-17-1.png" width="864" /></p>
<p>Os tópicos sobre os quais a Nathalia mais fala são Renda Fixa, Economia Doméstica e Dívidas. Quem acompanha seu canal não vai ficar surpreso com estes resultados, o que é um índicio da qualidade do modelo construído neste post.</p>
<p>Também é possível extrair os vídeos chaves de cada tópico, isto é, os vídeos com a maior probabilidade para cada tópico:</p>
<pre class="r"><code>df_topico %&gt;% 
  group_by(topico) %&gt;% 
  filter(maior_prob == max(maior_prob)) %&gt;% 
  select(id, title, topico, maior_prob) %&gt;% 
  knitr::kable()</code></pre>
<table>
<thead>
<tr class="header">
<th align="left">id</th>
<th align="left">title</th>
<th align="left">topico</th>
<th align="right">maior_prob</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">_alsKxI3-TI</td>
<td align="left">NATH AO VIVO! COMO VIAJAR SEM COLOCAR A MÃO NO BOLSO! Dicas preciosas dos viciados por pontos!</td>
<td align="left">cartao_de_credito</td>
<td align="right">0.9906875</td>
</tr>
<tr class="even">
<td align="left">eMQZRHoIgtQ</td>
<td align="left">COMO EU TIRO AS METAS DO PAPEL! Técnica simples pra juntar mais dinheiro do que nunca!</td>
<td align="left">plan_financeiro</td>
<td align="right">0.9773071</td>
</tr>
<tr class="odd">
<td align="left">FHqHW8-xvLQ</td>
<td align="left">Dia das crianças - COMO FALAR QUE A GRANA TÁ CURTA</td>
<td align="left">mulher_familia</td>
<td align="right">0.9717965</td>
</tr>
<tr class="even">
<td align="left">iA9Iqx2uByo</td>
<td align="left">Entrevista - Carlos Osório - Que Candidato é esse</td>
<td align="left">politica</td>
<td align="right">0.9907593</td>
</tr>
<tr class="odd">
<td align="left">kddZm50Gluw</td>
<td align="left">Perguntas e respostas #5 - Cartão de crédito e FIM DE NAMORO!</td>
<td align="left">dividas</td>
<td align="right">0.9629534</td>
</tr>
<tr class="even">
<td align="left">liIQENCBlF0</td>
<td align="left">SE VOCÊ NÃO ENTENDER ISSO, NUNCA VAI INVESTIR NA BOLSA!_ Renda variável do jeito mais simples</td>
<td align="left">renda_variavel</td>
<td align="right">0.9695903</td>
</tr>
<tr class="odd">
<td align="left">R0AQyTIvcvI</td>
<td align="left">O que é SELIC E CDI Entenda isso HOJE e pare de PERDER DINHEIRO! _ SÉRIE INVESTIDORES INICIANTES</td>
<td align="left">renda_fixa</td>
<td align="right">0.9899924</td>
</tr>
<tr class="even">
<td align="left">SfG934AzjYU</td>
<td align="left">COMO EU COMPRO ROUPAS DUAS VEZES AO ANO! Episódio final _ A negociação!</td>
<td align="left">economia_domestica</td>
<td align="right">0.9877744</td>
</tr>
<tr class="odd">
<td align="left">WtFV03Suheg</td>
<td align="left">A FINAL DA CONTRATAÇÃO QUE VOCÊ RESPEITA! Quem a Nath vai contratar (AGORA GRAVADO)</td>
<td align="left">generico</td>
<td align="right">0.9605720</td>
</tr>
<tr class="even">
<td align="left">Zla1-t3aOZw</td>
<td align="left">3 ESCOLHAS QUE ENRIQUECERAM O CERBASI _ meu guru</td>
<td align="left">empreendedorismo</td>
<td align="right">0.9151230</td>
</tr>
</tbody>
</table>
<p>Eu, pessoalmente, estou bastante satisfeito com os resultados. A modelagem de tópicos funcionou muito bem.</p>
<p>Vamos estão analisar algumas estatísticas básicas para cada tópico: quais têm os vídeos mais longos? Quais são os mais populares?</p>
<pre class="r"><code>meu_grafico &lt;- function(df, var){
  quo_var &lt;- enquo(var)

  mediana_geral &lt;- df %&gt;% 
    summarise_at(vars(!!quo_var), median, na.rm = TRUE) %&gt;% 
    pull()
  
  # obter grafico
  p &lt;- df %&gt;% 
    group_by(topico) %&gt;% 
    summarise(m = median(!!quo_var, na.rm = TRUE)) %&gt;% 
    ggplot(aes(x = topico, y = m)) +
    geom_col(fill = roxo) +
    geom_hline(yintercept = mediana_geral, linetype = &quot;dashed&quot;) +
    theme_minimal() +
    coord_flip() +
    labs(x =  NULL, y = NULL)
  
  p
}
  
meu_grafico(df_topico, duration) + ggtitle(&quot;Média da duração dos vídeos por tópico do Me Poupe&quot;)</code></pre>
<p><img src="/post/2018-04-14-topic-modeling-nathalia-arcuri_files/figure-html/graficos-1.png" width="864" /></p>
<pre class="r"><code>meu_grafico(df_topico, view_count) + ggtitle(&quot;Média de views por tópico do Me Poupe&quot;)</code></pre>
<p><img src="/post/2018-04-14-topic-modeling-nathalia-arcuri_files/figure-html/graficos-2.png" width="864" /></p>
<pre class="r"><code>meu_grafico(df_topico, like_count) + ggtitle(&quot;Média de likes por tópico do Me Poupe&quot;)</code></pre>
<p><img src="/post/2018-04-14-topic-modeling-nathalia-arcuri_files/figure-html/graficos-3.png" width="864" /></p>
<p>Com estes gráficos, aprendemos que os vídeos sobre renda fixa, planejamento financeiro e empreendedorismo são os mais populares entre os “me poupeiros” (como a Nathalia chama quem acompanha o canal), pois possuem mais visualizações e curtidas. Outro fato interessante é que os vídeos categorizados como política são, por assim dizer, improdutivos, pois, apesar de serem os mais longos, possuem muitos poucos views. Existem alguns possíveis tipos de viés que poderiam explicar este dado, como o temporal.</p>
</div>
<div id="analise-de-sentimento" class="section level3">
<h3>Análise de Sentimento</h3>
<p>Finalmente, vamos aplicar uma abordagem de <a href="http://sillasgonzaga.com/post/o-sensacionalista-e-text-mining/">análise de sentimento</a> para analisar se os tópicos apresentam diferentes polaridades entre si. Quais tópicos são mais negativos?</p>
<pre class="r"><code>data(&quot;sentiLex_lem_PT02&quot;)
dict &lt;- unique(sentiLex_lem_PT02)

# criacao do dataframe de sentimento por topico
df_sent &lt;- df %&gt;% 
  unnest_tokens(palavra, caption) %&gt;% 
  inner_join(sentiLex_lem_PT02, by = c(&quot;palavra&quot; = &quot;term&quot;)) %&gt;% 
  group_by(id) %&gt;% 
  summarise(
    sentimento_soma = sum(polarity),
    sentimento_media = mean(polarity)
  )

# acrescentar ao dafaframe principal
df_topico_sent &lt;- inner_join(df_topico, df_sent, by = &quot;id&quot;) 


# calcular sentimento geral dos videos
sent_medio_geral &lt;- median(df_topico_sent$sentimento_media)

df_topico_sent %&gt;% 
  mutate(topico = forcats::fct_reorder(topico, sentimento_media, median)) %&gt;% 
  ggplot(aes(x = sentimento_media, y = topico)) +
  geom_density_ridges(fill = roxo) +
  geom_vline(xintercept = sent_medio_geral, linetype = &quot;dashed&quot;) +
  theme_minimal() +
  labs(x = &quot;Sentimento&quot;, y = NULL,
       title = &quot;Distribuição dos sentimentos por tópico do Me Poupe&quot;)</code></pre>
<pre><code>## Picking joint bandwidth of 0.0756</code></pre>
<p><img src="/post/2018-04-14-topic-modeling-nathalia-arcuri_files/figure-html/analise_sent-1.png" width="864" /></p>
<p>Em novamente o que eu considero um acerto da metodologia, os vídeos de dívidas apresentaram polaridades mais baixas, ou sejas, um nível de sentimento mais negativo.</p>
</div>
</div>
<div id="conclusao" class="section level2">
<h2>Conclusão</h2>
<p>É relativamente difícil avaliar a acurácia e precisão de um algoritmo de topic modeling por essa técnica ser não-supervisionada, ou seja, os dados não possuírem um output com a resposta correta.</p>
<p>Contudo, os resultados apresentados neste post mostram que é possível sim usar esse tipo de técnica, com um certo auxílio de interpretação humana, para categorizar um conjunto de vídeos sem ser necessário os assistir ou mesmo sem saber seus títulos.</p>
</div>
<div id="referencias" class="section level2">
<h2>Referências</h2>
<p>Antes das referências formais, gostaria de indicar o post do <a href="http://curso-r.com/blog/2018/02/23/2018-02-21-2cents/">Júlio Trecenti</a>, que é um outro exemplo de uso criativo de análise de dados para youtubers.</p>
<p><a href="https://en.wikipedia.org/wiki/Topic_model" class="uri">https://en.wikipedia.org/wiki/Topic_model</a></p>
<p>Mimno, D., Wallach, H. M., Talley, E., Leenders, M., &amp; McCallum, A. (2011, July). “Optimizing semantic coherence in topic models.” In Proceedings of the Conference on Empirical Methods in Natural Language Processing (pp. 262-272). Association for Computational Linguistics. Chicago</p>
<p>Roberts, M., Stewart, B., Tingley, D., Lucas, C., Leder-Luis, J., Gadarian, S., Albertson, B., et al. (2014). “Structural topic models for open ended survey responses.” American Journal of Political Science, 58(4), 1064-1082</p>
<p>Margaret E. Roberts, Brandon M. Stewart and Dustin Tingley (2018). stm: R Package for Structural Topic Models. URL <a href="http://www.structuraltopicmodel.com" class="uri">http://www.structuraltopicmodel.com</a>.</p>
</div>
